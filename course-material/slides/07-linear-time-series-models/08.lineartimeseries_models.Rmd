---
title: "FIN7028: Times Series Financial Econometrics"
author: "Linear time series models"
date: ""
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 8
    fig_height: 5
    highlight: tango
    theme: metropolis
    includes:
      in_header: headerQub.tex
---


```{r setup, include=FALSE}
MyDeleteItems<-ls()
rm(list=MyDeleteItems)
knitr::opts_knit$set(root.dir='~/Dropbox/Teaching/TSFE/FIN7028')
knitr::opts_chunk$set(echo = FALSE,cache=TRUE,warning = FALSE, message = FALSE)
# library(fpp3)
library(fpp2)
theme_set(theme_dark(base_size = 14))
load("~/Dropbox/Teaching/TSFE/FIN7028/data/week7_dat.RData") 
# all<-ls()
# keep<-c('vix_ts','r2000r_m_ts',"indices_m","ftse_m_ts","next_m_tr_ts","next_m_tr","next_factormodel_ts")
# rm(list=setdiff(all,keep)) # This keeps only the data that is required for this workshop

```

# Recap

## Last week recap

### Linear time series models so far
1. Decomposition of time series
2. Seasonal adjustment
3. Forecasting and decomposition
4. Stationarity and differencing
5. Non-seasonal ARIMA models
6. Estimation and order selection

# ARIMA modelling in R

## How does auto.arima() work?

\begin{block}{A non-seasonal ARIMA process}
\[
\phi(B)(1-B)^dy_{t} = c + \theta(B)\varepsilon_t
\]
Need to select appropriate orders: \alert{$p,q, d$}
\end{block}

\structure{Hyndman and Khandakar (JSS, 2008) algorithm:}

  * Select no.\ differences \alert{$d$} and \alert{$D$} via KPSS test and seasonal strength measure.
  * Select \alert{$p,q$} by minimising AICc.
  * Use stepwise search to traverse model space.

## How does auto.arima() work?
\fontsize{12}{13}\sf

\begin{block}{}
\centerline{$\text{AICc} = -2 \log(L) + 2(p+q+k+1)\left[1 +
\frac{(p+q+k+2)}{T-p-q-k-2}\right].$}
where $L$ is the maximised likelihood fitted to the \textit{differenced} data,
$k=1$ if $c\neq 0$ and $k=0$ otherwise.
\end{block}\pause

Step1:
:  Select current model (with smallest AICc) from:\newline
ARIMA$(2,d,2)$\newline
ARIMA$(0,d,0)$\newline
ARIMA$(1,d,0)$\newline
ARIMA$(0,d,1)$
\pause\vspace*{-0.1cm}

Step 2:
:  Consider variations of current model:

    * vary one of $p,q,$ from current model by $\pm1$;
    * $p,q$ both vary from current model by $\pm1$;
    * Include/exclude $c$ from current model.

  Model with lowest AICc becomes current model.

\structure{Repeat Step 2 until no lower AICc can be found.}

## Choosing your own model: VIX index

```{r, echo=TRUE, fig.height=4}
ggtsdisplay(log(vix_ts))
```

## Choosing your own model: VIX index

```{r, echo=TRUE, fig.height=4}
ggtsdisplay(diff(log(vix_ts)))
```

## Choosing your own model: VIX index
\fontsize{8}{14}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- Arima(log(vix_ts),order=c(7,1,0)))
```

## Own model vs `auto.arima()`: VIX index
\fontsize{13}{14}\sf

```{r, echo=TRUE, fig.height=4}
auto.arima(log(vix_ts))
```

## Own model versus `auto.arima`: VIX index
\fontsize{10}{14}\sf

```{r tryharder, echo=TRUE, fig.height=4}
(fit.auto<-auto.arima(log(vix_ts), 
                      stepwise=FALSE,
                      approximation=FALSE))
```

## Own model versus `auto.arima`: VIX index

* Setting both `stepwise` and `approximation` arguments to `FALSE` will slow the automation down but provides a more exhaustive search for the appropriate model.
* The `auto.arima` function then searches over all possible models using MLE.
* See `help(auto.arima)` for more details.

## Own model versus `auto.arima`: VIX

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, test=FALSE)
```


## Own model versus `auto.arima`: VIX

```{r, echo=TRUE}
checkresiduals(fit, plot=FALSE)
```


## Own model versus `auto.arima`: VIX

```{r, echo=TRUE, fig.height=4}
fit %>% forecast(h=252) %>% autoplot
```

## Own model versus `auto.arima`: VIX

```{r echo=TRUE, fig.height=4}
checkresiduals(fit.auto,test = FALSE)
```


## Own model versus `auto.arima`: VIX

```{r echo=TRUE, fig.height=4}
checkresiduals(fit.auto,plot = FALSE)
```

## Own model versus `auto.arima`: VIX

```{r echo=TRUE, fig.height=4}
fit.auto %>% forecast(h=252) %>% autoplot()
```


## Modelling procedure with `Arima`
\fontsize{12}{13}\sf

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.
3. If the data are non-stationary: take first differences of the data until the data are stationary.
4. Examine the ACF/PACF: Is an AR($p$) or MA($q$) model appropriate?
5. Try your chosen model(s), and  use the \text{AICc} to search for a better model.
6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.


## Modelling procedure with `auto.arima`
\fontsize{12}{13}\sf

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.

\vspace*{1.15cm}

3. Use `auto.arima` to select a model.

\vspace*{1.15cm}

6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

## Modelling procedure

\centerline{\includegraphics[height=8.cm]{Figure-8-10}}


## Russell 2000 index monthly returns
\fontsize{11.5}{15}\sf

```{r r2000r1, fig.height=3.3, echo=TRUE}
autoplot(r2000r_m_ts) + xlab("Year") +
  ylab("monthly log returns")
```


## Russell 2000 index monthly returns

1. Time plot shows sudden changes, particularly big movements in 2007/2008 due to financial crisis. Otherwise nothing unusual and no need for  data adjustments.
2. Little evidence of changing variance, so a Box-Cox transformation not necessary.
3. Data are clearly stationary, so no differencing required.


## Russell 2000 index monthly returns

```{r r2000r2, echo=TRUE, fig.height=4}
ggtsdisplay(r2000r_m_ts)
```

## Russell 2000 index monthly returns

4. PACF is suggestive of AR(5). So initial candidate model is ARIMA(5,0,0). No other obvious candidates.
5. Fit ARIMA(5,0,0) model along with variations: ARIMA(4,0,0), ARIMA(3,0,0), ARIMA(4,0,1), etc. ARIMA(3,0,1) has smallest \text{AICc} value.

## Russell 2000 index monthly returns
\fontsize{10}{10}\sf

```{r r2000r3, echo=TRUE}
(fit <- Arima(r2000r_m_ts, order=c(3,0,1)))
```

## Russell 200 index monthly returns

6. ACF plot of residuals from ARIMA(3,0,1) model look like white noise.

\fontsize{11}{14}\sf

```r
checkresiduals(fit)
```

```{r r2000r4, echo=FALSE, fig.height=3.4}
checkresiduals(fit, test = FALSE)
```

## Russell 200 index monthly returns

```{r r20005, echo=FALSE}
checkresiduals(fit, plot=FALSE)
```

## Russell 200 index monthly returns

```{r, echo=TRUE}
fit %>% forecast(h=24) %>% autoplot
```

## Understanding the ARIMA output

\fontsize{10}{14}\sf

```{r r2000r6}
(fit <- Arima(r2000r_m_ts, order=c(3,0,1)))
```

## Understanding the ARIMA output
\fontsize{10}{14}\sf

```{r coeffs}
a=unname(round(fit$coef["intercept"],3))
ar1=unname(round(fit$coef["ar1"],3))
ar2=unname(round(fit$coef["ar2"],3))
ar3=unname(round(fit$coef["ar3"],3))
ma1=unname(round(fit$coef["ma1"],3))
```

* The fitted model is:

$$y_t=`r ar1`y_{t-1}`r ar2`y_{t-2}`r ar3`y_{t-3} +`r ma1`\varepsilon_{t-1}+0.008$$

* The standard errors are 0.13, 0.06, 0.05, 0.12 and 0.002, respectively.
* This suggest that only the AR1 and the constant (mean) are more than 2 SEs away from zero and thus statistically significant.
* The significance of $\phi_0$ of this entertained model implies that the expected mean return of the series is positive.

* In fact $\hat{\mu}=`r a`/(1-(`r ar1``r ar2``r ar3`)) =`r round(a/(1-sum(ar1,ar2,ar3)),4)`$ which is small but has long term implications.

* Using the multi-period return definition from the financial data lecture an annualised log return is simple $\sum_1^{12} y_t$ $\approx `r round(12*a/(1-sum(ar1,ar2,ar3)),2)`$ per annum. 

# Forecasting

## Point forecasts

* How do we calculate point forecasts from ARIMA models?

1. Rearrange ARIMA equation so $y_t$ is on LHS.
2. Rewrite equation by replacing $t$ by $T+h$.
3. On RHS, replace future observations by their forecasts, future errors by zero, and past errors by corresponding residuals.

Start with $h=1$. Repeat for $h=2,3,\dots$.

## Point forecasts
\fontsize{14}{14}\sf

\structure{ARIMA(3,1,1) forecasts: Step 1}
\begin{block}{}
\centerline{$(1-\phi_1B -\phi_2B^2-\phi_3B^3)(1-B) y_t = (1+\theta_1B)\varepsilon_{t},$}
\end{block}
\pause\vspace*{-0.4cm}
\begin{align*}
\left[1-(1+\phi_1)B +(\phi_1-\phi_2)B^2 + (\phi_2-\phi_3)B^3 +\phi_3B^4\right] y_t\\ = (1+\theta_1B)\varepsilon_{t},
\end{align*}\pause\vspace*{-0.4cm}
\begin{align*}
y_t - (1+\phi_1)y_{t-1} +(\phi_1-\phi_2)y_{t-2} + (\phi_2-\phi_3)y_{t-3}\\ \mbox{}+\phi_3y_{t-4} = \varepsilon_t+\theta_1\varepsilon_{t-1}.
\end{align*}\pause\vspace*{-0.4cm}
\begin{align*}
y_t = (1+\phi_1)y_{t-1} -(\phi_1-\phi_2)y_{t-2} - (\phi_2-\phi_3)y_{t-3}\\\mbox{} -\phi_3y_{t-4} + \varepsilon_t+\theta_1\varepsilon_{t-1}.
\end{align*}

## Point forecasts (h=1)
\fontsize{14}{14}\sf

\begin{block}{}
\begin{align*}
y_t = (1+\phi_1)y_{t-1} -(\phi_1-\phi_2)y_{t-2} - (\phi_2-\phi_3)y_{t-3}\\\mbox{} -\phi_3y_{t-4} + \varepsilon_t+\theta_1\varepsilon_{t-1}.
\end{align*}
\end{block}\pause
\structure{ARIMA(3,1,1) forecasts: Step 2}
\begin{align*}
y_{T+1} = (1+\phi_1)y_{T} -(\phi_1-\phi_2)y_{T-1} - (\phi_2-\phi_3)y_{T-2}\\\mbox{} -\phi_3y_{T-3} + \varepsilon_{T+1}+\theta_1\varepsilon_{T}.
\end{align*}\pause
\structure{ARIMA(3,1,1) forecasts: Step 3}
\begin{align*}
\hat{y}_{T+1|T} = (1+\phi_1)y_{T} -(\phi_1-\phi_2)y_{T-1} - (\phi_2-\phi_3)y_{T-2}\\\mbox{} -\phi_3y_{T-3} + \theta_1 e_{T}.
\end{align*}

## Point forecasts (h=2)
\fontsize{14}{14}\sf

\begin{block}{}
\begin{align*}
y_t = (1+\phi_1)y_{t-1} -(\phi_1-\phi_2)y_{t-2} - (\phi_2-\phi_3)y_{t-3}\\\mbox{} -\phi_3y_{t-4} + \varepsilon_t+\theta_1\varepsilon_{t-1}.
\end{align*}
\end{block}\pause

\structure{ARIMA(3,1,1) forecasts: Step 2}
\begin{align*}
y_{T+2} = (1+\phi_1)y_{T+1} -(\phi_1-\phi_2)y_{T} - (\phi_2-\phi_3)y_{T-1}\\\mbox{} -\phi_3y_{T-2} + \varepsilon_{T+2}+\theta_1\varepsilon_{T+1}.
\end{align*}\pause

\structure{ARIMA(3,1,1) forecasts: Step 3}
\begin{align*}
\hat{y}_{T+2|T} = (1+\phi_1)\hat{y}_{T+1|T} -(\phi_1-\phi_2)y_{T} - (\phi_2-\phi_3)y_{T-1}\\\mbox{} -\phi_3y_{T-2}.
\end{align*}


## Prediction intervals

\begin{block}{95\% prediction interval}
$$\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}$$
where $v_{T+h|T}$ is estimated forecast variance.
\end{block}\pause

* $v_{T+1|T}=\hat{\sigma}^2$ for all ARIMA models regardless of parameters and orders.
* Multi-step prediction intervals for ARIMA(0,0,$q$):
\centerline{$\displaystyle y_t = \varepsilon_t + \sum_{i=1}^q \theta_i \varepsilon_{t-i}$}
\centerline{$\displaystyle v_{T|T+h} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \theta_i^2\right], \qquad\text{for~} h=2,3,\dots.$}

## Frequentist prediction intervals

\begin{block}{95\% Prediction interval}
$$\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}$$
where $v_{T+h|T}$ is estimated forecast variance.
\end{block}

* Multi-step prediction intervals for ARIMA(0,0,$q$):
\centerline{$\displaystyle y_t = \varepsilon_t + \sum_{i=1}^q \theta_i \varepsilon_{t-i}.$}
\centerline{$\displaystyle
v_{T|T+h} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \theta_i^2\right], \qquad\text{for~} h=2,3,\dots.$}

\pause

* AR(1): Rewrite as MA($\infty$) and use above result.
* Other models beyond scope of this subject.


## Prediction intervals

* Prediction intervals **increase in size with forecast horizon**.
* Prediction intervals can be difficult to calculate by hand
* Calculations assume residuals are **uncorrelated** and **normally distributed**.
* Prediction intervals tend to be too narrow.
    * the uncertainty in the parameter estimates has not been accounted for.
    * the ARIMA model assumes historical patterns will not change during the forecast period.
    * the ARIMA model assumes uncorrelated future \rlap{errors}



# Seasonal ARIMA models

## Seasonal ARIMA models

| ARIMA | $~\underbrace{(p, d, q)}$ | $\underbrace{(P, D, Q)_{m}}$ |
| ----: | :-----------------------: | :--------------------------: |
|       | ${\uparrow}$              | ${\uparrow}$                 |
|       | Non-seasonal part         | Seasonal part of             |
|       | of the model              | of the model                 |


where $m =$ number of observations per year.



## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)\pause
$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
$$\pause

\setlength{\unitlength}{1mm}
\begin{footnotesize}
\begin{picture}(100,25)(-5,0)
\thinlines
{\put(5,22){\vector(0,1){6}}}
{\put(22,10){\vector(0,1){18}}}
{\put(38,22){\vector(0,1){6}}}
{\put(52,10){\vector(0,1){18}}}
{\put(77,22){\vector(0,1){6}}}
{\put(95,10){\vector(0,1){18}}}
{\put(-10,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(12,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(25,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(40,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(65,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
{\put(85,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
\end{picture}
\end{footnotesize}

\vspace*{10cm}


## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)

$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} = (1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.$$

All the factors can be multiplied out and the general model
written as follows:
\begin{align*}
y_{t}  &= (1 + \phi_{1})y_{t - 1} - \phi_1y_{t-2} + (1 + \Phi_{1})y_{t - 4}\\
&\text{}-(1  + \phi_{1}  +  \Phi_{1} + \phi_{1}\Phi_{1})y_{t - 5}+(\phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 6} \\
& \text{}  - \Phi_{1} y_{t - 8} +  (\Phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 9}
- \phi_{1} \Phi_{1} y_{t  -  10}\\
&\text{}+\varepsilon_{t} + \theta_{1}\varepsilon_{t - 1} + \Theta_{1}\varepsilon_{t - 4}  + \theta_{1}\Theta_{1}\varepsilon_{t - 5}.
\end{align*}
\vspace*{10cm}


## Common ARIMA models

The US Census Bureau uses the following models most often:\vspace*{0.5cm}

\begin{tabular}{|ll|}
\hline
ARIMA(0,1,1)(0,1,1)$_m$& with log transformation\\
ARIMA(0,1,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,0)(0,1,1)$_m$& with log transformation\\
ARIMA(0,2,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,2)(0,1,1)$_m$& with no transformation\\
\hline
\end{tabular}


## Seasonal ARIMA models
The seasonal part of an AR or MA model will be seen in the seasonal lags of
the PACF and ACF.

\structure{ARIMA(0,0,0)(0,0,1)$_{12}$ will show:}

  * a spike at lag 12 in the ACF but no other significant spikes.
  * The PACF will show exponential decay in the seasonal lags;
     that is, at lags 12, 24, 36, \dots.

\structure{ARIMA(0,0,0)(1,0,0)$_{12}$ will show:}

  *  exponential decay in the seasonal lags of the ACF
  * a single significant spike at lag 12 in the PACF.

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.6}
autoplot(euretail) +
  xlab("Year") + ylab("Retail index")
```

## European quarterly retail trade

```{r, echo=TRUE, fig.height=4}
euretail %>% diff(lag=4) %>% ggtsdisplay()
```

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.8}
euretail %>% diff(lag=4) %>% diff() %>%
  ggtsdisplay()
```

## European quarterly retail trade

  * $d=1$ and $D=1$ seems necessary.
  * Significant spike at lag 1 in ACF suggests non-seasonal MA(1) component.
  * Significant spike at lag 4 in ACF suggests seasonal MA(1) component.
  * Initial candidate model: ARIMA(0,1,1)(0,1,1)$_4$.
  * We could also have started with ARIMA(1,1,0)(1,1,0)$_4$.

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.5}
fit <- Arima(euretail, order=c(0,1,1),
             seasonal=c(0,1,1))
checkresiduals(fit)
```

## European quarterly retail trade

\fontsize{13}{14}\sf

```{r, echo=FALSE}
checkresiduals(fit, plot=FALSE) 
```

## European quarterly retail trade

```{r, echo=FALSE}
aicc <- c(
  Arima(euretail, order=c(0,1,2), seasonal=c(0,1,1))$aicc,
  Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1))$aicc)
```

  * ACF and PACF of residuals show significant spikes at lag 2, and maybe lag 3.
  * AICc of ARIMA(0,1,2)(0,1,1)$_4$ model is `r round(aicc[1],2)`.
  * AICc of ARIMA(0,1,3)(0,1,1)$_4$ model is `r round(aicc[2],2)`.
\pause\vfill

```r
fit <- Arima(euretail, order=c(0,1,3),
  seasonal=c(0,1,1))
checkresiduals(fit)
```

## European quarterly retail trade

\fontsize{12}{15}\sf

```{r}
(fit <- Arima(euretail, order=c(0,1,3),
              seasonal=c(0,1,1)))
```

## European quarterly retail trade
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit)
```

## European quarterly retail trade
\fontsize{13}{15}\sf

```{r, echo=FALSE}
checkresiduals(fit, plot=FALSE)
```

## European quarterly retail trade

```{r, echo=TRUE, fig.height=4}
autoplot(forecast(fit, h=12))
```

## European quarterly retail trade
\fontsize{12}{14}\sf

```{r, echo=TRUE}
auto.arima(euretail)
```

## European quarterly retail trade
\fontsize{12}{14}\sf

```{r euretailtryharder, echo=TRUE}
auto.arima(euretail,
          stepwise=FALSE, approximation=FALSE)
```



# Unit-root nonstationary models

## Unit root stationarity.

* When we study returns we are mostly studying stationary time series.
* Sometimes we are interested in a foreign exchange rate, or the price series of an asset.
* These series tend to be non-stationary.
* A price series is non-stationary due to the fact that there is no fixed price level; **it is a unit-root nonstationary time series**

## Random walk model

* This is the best known unit-root nonstationary time series model.
* For a price series $p_t$:

$$p_t=p_{t-1}+\varepsilon_t$$ 

* $\varepsilon_t$ is a white noise term
* This formula implies that if $\varepsilon_t$ is distributed symmetrically around 0, then conditional on $p_{t-1}$, $p_t$ has a 50-50 chance to go up or down.

## Random walk model

* Assume we have a log price series $p_t$ where the first observation $p_0=0$.
* If we assume a random walk how will the prices series evolve?
* The following output simulates 5 possible price paths using a previous random walk model.

## Random walk model simulation

\includegraphics{rw1.png}


## Random walk model with drift

* In most empirical studies of the log returns of a market index, they tend to have a small and positive mean.
* For example the average monthly return in the `indices_m.RData` are:

\fontsize{10}{14}\sf

```{r meanindicesreturns,out.height="30%"}
library(tidyr)
library(dplyr)
indice_m_long <-indices_m %>%
  gather(Index,Price,-date) %>%
  arrange(Index,date) %>%
  group_by(Index) %>%
  mutate(log_return=log(Price)-log(lag(Price)))

library(knitr)
library(kableExtra)
indice_m_long %>%
  group_by(Index) %>%
  summarise("Mean Log Return"=mean(log_return,na.rm = T)) %>%
  slice(-c(5:7,9,12:17)) %>%
  kable( digits = 5) %>%
  kable_styling(bootstrap_options = "striped",full_width = T)
```

## Random walk model with drift

* This implies that the model of log prices is

\begin{align*}
p_t= \mu + p_{t-1}+\varepsilon_t \\ \varepsilon_t=\text{white noise}
\end{align*}

* where $\mu=E(p_t- p_{t-1})$, this is the *drift* term which represents the time trend of the log price $p_t$

## Random walk with drift example: NEXT plc

```{r next total returns,out.height="50%",fig.align="center"}
ggtsdisplay(next_m_tr_ts)
```

\small
* To illustrate the effect of the drift parameter on a price series we consider the monthly log returns for NEXT plc from January 2001 to May 2019.
* The above plot shows no significant autocorrelation (the PACF suggests no AR terms)
* The series can thus be estimated using an arima(0,0,0) with a constant

## Random walk with drift example: NEXT plc 
\fontsize{10}{14}\sf
```{r next2, echo=TRUE}
(fit<-Arima(next_m_tr_ts , order = c(0,0,0),
            include.mean = T ))
```

## Random walk with drift example: NEXT plc 

* The model is: 
$$y_t=`r round(fit$coef["intercept"],3)`+\varepsilon_t, \sigma_{varepsilon}=`r round(sqrt(fit$sigma2),2)`$$
* where `r round(fit$coef["intercept"],3)` is the sample mean of the monthly log returns and has a standard error of 0.005.
* As the coefficient estimate is more than 2 standard errors away from zero we can also conclude that it is statistically significant (given the model is correct).

## Random walk with drift example: NEXT plc

* To illustrate how this drift term works we construct two new log prices series from NEXT plc prices.

$$p_t=\sum_{i=1}^{T}logprice_i$$ 

$$p_t^*=\sum_{i=1}^T(logprice_i-0.012)$$

* In both cases we assume the log price=0 at i=1

## Random walk with drift example: NEXT plc

```{r next3, out.height="70%",fig.align="center"}
next_m_tr %>%
  mutate(t=1:n(),
        Trendline=0.0118*t,
         Demeaned=cumsum(scale(MonthlyTotalReturn,scale = FALSE)),
        Ln_price=cumsum(MonthlyTotalReturn)) %>%
  select(DateMonth,Trendline,Demeaned,Ln_price) %>%
  gather(Series,Price,-DateMonth) %>%
  ggplot(aes(x=DateMonth,y=Price,colour=Series)) +
  geom_line() +
    labs(x="Year",y="ln(price)",title ="Plot of log prices for Next stock from Jan 2001 to May 2019", subtitle = bquote("trendline:" ~ y[t] == 0.012 * t))
```

* The constant term represents the slope of the upward trend in $p_t$

## Interpretation of the constant term

* For $MA(q)$ model it is the mean of the series
* For a stationary $AR(p)$ model or an $ARMA(p,q)$ model recall the mean is $\mu=\phi_0/(1-\phi_1 \dots - \phi_q)$
* For a random walk with drift it becomes the time slope of the series.

## Trend-stationary time series
\begin{block}{trend-stationary time series model}
$$p_{t} = \beta_0 +\beta_1t+ y_t, \text{where } y_t \text{ is stationary}$$
\end{block}
\small
* Looks similar to a random walk with drift, but this model has a major difference from a random walk with drift.
* A RW+drift model has a mean $E(p_t)=p_0+\mu t$ and variance $Var(p_t)=t\sigma_\varepsilon^2$ which are time dependent.
* A trend stationary model has a time dependent mean of $E(p_t)=\beta_0+\beta_1t$ but a time invariant and finite variance $Var(p_t)=Var(y_t)$
* Trend-stationary series can be transformed to stationary by removing the trend using a simple linear regression.
